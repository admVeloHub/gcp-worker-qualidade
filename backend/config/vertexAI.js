// VERSION: v1.2.0 | DATE: 2025-01-30 | AUTHOR: VeloHub Development Team
const speech = require('@google-cloud/speech');
const { GoogleGenerativeAI } = require('@google/generative-ai');
const { getSecret } = require('./secrets');

// Configura√ß√£o
const GCP_PROJECT_ID = process.env.GCP_PROJECT_ID;
const GCP_LOCATION = process.env.GCP_LOCATION || 'us-central1';
const GCS_BUCKET_NAME = process.env.GCS_BUCKET_NAME;

// Inicializar clientes
let speechClient;
let genAI;
let geminiApiKey;

/**
 * Detectar encoding de √°udio baseado na extens√£o do arquivo
 * @param {string} fileName - Nome do arquivo com extens√£o
 * @returns {Object} { encoding: string, sampleRateHertz: number }
 */
const detectAudioEncoding = (fileName) => {
  const extension = fileName.toLowerCase().split('.').pop();
  
  switch (extension) {
    case 'mp3':
      return {
        encoding: 'MP3',
        sampleRateHertz: 44100
      };
    case 'wav':
      return {
        encoding: 'LINEAR16',
        sampleRateHertz: 16000
      };
    default:
      // Fallback para WEBM_OPUS
      console.warn(`‚ö†Ô∏è  Formato de √°udio n√£o reconhecido (${extension}), usando WEBM_OPUS como fallback`);
      return {
        encoding: 'WEBM_OPUS',
        sampleRateHertz: 16000
      };
  }
};

/**
 * Inicializar clientes Vertex AI
 */
const initializeVertexAI = async () => {
  try {
    if (!GCP_PROJECT_ID) {
      throw new Error('GCP_PROJECT_ID deve estar configurado nas vari√°veis de ambiente');
    }

    // Inicializar Speech-to-Text client
    if (!speechClient) {
      speechClient = new speech.SpeechClient({
        projectId: GCP_PROJECT_ID
      });
    }

    // Buscar GEMINI_API_KEY do Secret Manager se ainda n√£o foi carregada
    if (!geminiApiKey) {
      try {
        geminiApiKey = await getSecret('GEMINI_API_KEY');
      } catch (error) {
        throw new Error(`Falha ao buscar GEMINI_API_KEY do Secret Manager: ${error.message}`);
      }
    }

    // Inicializar Gemini AI
    if (!genAI && geminiApiKey) {
      genAI = new GoogleGenerativeAI(geminiApiKey);
    } else if (!genAI) {
      throw new Error('GEMINI_API_KEY deve estar configurada no Secret Manager');
    }

    console.log('‚úÖ Vertex AI inicializado');
    return { speechClient, genAI };
  } catch (error) {
    console.error('‚ùå Erro ao inicializar Vertex AI:', error);
    throw error;
  }
};

/**
 * Transcrever √°udio usando Speech-to-Text
 * @param {string} gcsUri - URI do arquivo no GCS (gs://bucket/file)
 * @param {string} fileName - Nome do arquivo para detectar encoding
 * @param {string} languageCode - C√≥digo do idioma (ex: 'pt-BR')
 * @returns {Promise<{transcription: string, timestamps: Array}>}
 */
const transcribeAudio = async (gcsUri, fileName, languageCode = 'pt-BR') => {
  try {
    if (!speechClient) {
      await initializeVertexAI();
    }

    // Detectar encoding baseado na extens√£o do arquivo
    const audioConfig = detectAudioEncoding(fileName);
    
    const request = {
      audio: {
        uri: gcsUri
      },
      config: {
        encoding: audioConfig.encoding,
        sampleRateHertz: audioConfig.sampleRateHertz,
        languageCode: languageCode,
        enableAutomaticPunctuation: true,
        enableWordTimeOffsets: true, // Para timestamps
        model: 'latest_long', // Modelo otimizado para √°udios longos
        useEnhanced: true
      }
    };

    console.log(`üé§ Transcrevendo √°udio: ${gcsUri} (encoding: ${audioConfig.encoding}, sampleRate: ${audioConfig.sampleRateHertz}Hz)`);
    const [operation] = await speechClient.longRunningRecognize(request);
    
    // Aguardar conclus√£o da opera√ß√£o
    const [response] = await operation.promise();
    
    // Processar resultados
    let transcription = '';
    const timestamps = [];
    
    if (response.results && response.results.length > 0) {
      response.results.forEach(result => {
        if (result.alternatives && result.alternatives[0]) {
          const alternative = result.alternatives[0];
          transcription += alternative.transcript + ' ';
          
          // Extrair timestamps das palavras
          if (alternative.words) {
            alternative.words.forEach(word => {
              timestamps.push({
                word: word.word,
                startTime: word.startTime.seconds + word.startTime.nanos / 1e9,
                endTime: word.endTime.seconds + word.endTime.nanos / 1e9
              });
            });
          }
        }
      });
    }

    transcription = transcription.trim();

    console.log(`‚úÖ Transcri√ß√£o conclu√≠da: ${transcription.length} caracteres`);
    
    return {
      transcription,
      timestamps,
      confidence: response.results[0]?.alternatives[0]?.confidence || 0
    };
  } catch (error) {
    console.error('‚ùå Erro ao transcrever √°udio:', error);
    throw error;
  }
};

/**
 * Analisar emo√ß√£o e nuance usando Gemini
 * @param {string} transcription - Texto transcrito
 * @param {Array} timestamps - Timestamps das palavras
 * @returns {Promise<{emotion: object, nuance: object, analysis: string}>}
 */
const analyzeEmotionAndNuance = async (transcription, timestamps) => {
  try {
    if (!genAI) {
      await initializeVertexAI();
    }

    // Preparar prompt para an√°lise de emo√ß√£o e nuance
    const prompt = `
Analise a seguinte transcri√ß√£o de uma liga√ß√£o de atendimento e forne√ßa:

1. AN√ÅLISE DE EMO√á√ÉO E NUANCE:
   - Tom de voz (positivo, neutro, negativo)
   - N√≠vel de empatia demonstrado
   - Clareza na comunica√ß√£o
   - Profissionalismo
   - Pontos de tens√£o ou desconforto

2. AVALIA√á√ÉO DOS CRIT√âRIOS DE QUALIDADE:
   Avalie cada crit√©rio abaixo como true ou false baseado na transcri√ß√£o:

   - saudacaoAdequada: O colaborador cumprimentou adequadamente?
   - escutaAtiva: Demonstrou escuta ativa e fez perguntas relevantes?
   - clarezaObjetividade: Foi claro e objetivo na comunica√ß√£o?
   - resolucaoQuestao: Resolveu a quest√£o seguindo procedimentos?
   - dominioAssunto: Demonstrou conhecimento sobre o assunto?
   - empatiaCordialidade: Demonstrou empatia e cordialidade?
   - direcionouPesquisa: Direcionou para pesquisa de satisfa√ß√£o?
   - procedimentoIncorreto: Repassou informa√ß√£o incorreta? (true = negativo)
   - encerramentoBrusco: Encerrou o contato de forma brusca? (true = negativo)

3. PONTUA√á√ÉO:
   Calcule pontua√ß√£o de 0-100 baseado nos crit√©rios:
   - Crit√©rios positivos: +10 a +25 pontos cada
   - Crit√©rios negativos: -60 a -100 pontos cada

4. PALAVRAS-CHAVE CR√çTICAS:
   Liste palavras ou frases que indicam problemas ou pontos de aten√ß√£o.

TRANSCRI√á√ÉO:
${transcription}

Retorne um JSON com a seguinte estrutura:
{
  "analiseGPT": "An√°lise completa detalhada",
  "criteriosGPT": {
    "saudacaoAdequada": boolean,
    "escutaAtiva": boolean,
    "clarezaObjetividade": boolean,
    "resolucaoQuestao": boolean,
    "dominioAssunto": boolean,
    "empatiaCordialidade": boolean,
    "direcionouPesquisa": boolean,
    "procedimentoIncorreto": boolean,
    "encerramentoBrusco": boolean
  },
  "pontuacaoGPT": number,
  "confianca": number,
  "palavrasCriticas": ["palavra1", "palavra2"],
  "calculoDetalhado": ["explica√ß√£o1", "explica√ß√£o2"],
  "emotion": {
    "tom": "positivo|neutro|negativo",
    "empatia": number,
    "profissionalismo": number
  },
  "nuance": {
    "clareza": number,
    "tensao": number
  }
}
`;

    // Usar Gemini para an√°lise (j√° inicializado acima)

    const model = genAI.getGenerativeModel({ model: 'gemini-pro' });
    const result = await model.generateContent(prompt);
    const response = await result.response;
    const text = response.text();

    // Extrair JSON da resposta
    const jsonMatch = text.match(/\{[\s\S]*\}/);
    if (!jsonMatch) {
      throw new Error('Resposta do Gemini n√£o cont√©m JSON v√°lido');
    }

    const analysis = JSON.parse(jsonMatch[0]);

    console.log('‚úÖ An√°lise de emo√ß√£o e nuance conclu√≠da');
    
    return {
      emotion: analysis.emotion || {},
      nuance: analysis.nuance || {},
      analysis: analysis.analiseGPT || '',
      criteriosGPT: analysis.criteriosGPT || {},
      pontuacaoGPT: analysis.pontuacaoGPT || 0,
      confianca: analysis.confianca || 0,
      palavrasCriticas: analysis.palavrasCriticas || [],
      calculoDetalhado: analysis.calculoDetalhado || []
    };
  } catch (error) {
    console.error('‚ùå Erro ao analisar emo√ß√£o e nuance:', error);
    throw error;
  }
};

/**
 * Cruzar outputs de transcri√ß√£o e an√°lise de emo√ß√£o
 * @param {object} transcriptionResult - Resultado da transcri√ß√£o
 * @param {object} emotionResult - Resultado da an√°lise de emo√ß√£o
 * @returns {object} Resultado cruzado
 */
const crossReferenceOutputs = (transcriptionResult, emotionResult) => {
  try {
    // Cruzar timestamps com an√°lise de emo√ß√£o
    const crossReferenced = {
      transcription: transcriptionResult.transcription,
      timestamps: transcriptionResult.timestamps,
      emotion: emotionResult.emotion,
      nuance: emotionResult.nuance,
      qualityAnalysis: {
        criterios: emotionResult.criteriosGPT,
        pontuacao: emotionResult.pontuacaoGPT,
        confianca: emotionResult.confianca,
        palavrasCriticas: emotionResult.palavrasCriticas,
        calculoDetalhado: emotionResult.calculoDetalhado
      },
      analysis: emotionResult.analysis
    };

    console.log('‚úÖ Outputs cruzados com sucesso');
    
    return crossReferenced;
  } catch (error) {
    console.error('‚ùå Erro ao cruzar outputs:', error);
    throw error;
  }
};

/**
 * Retry com exponential backoff
 * @param {Function} fn - Fun√ß√£o a ser executada
 * @param {number} maxRetries - N√∫mero m√°ximo de tentativas
 * @param {number} baseDelay - Delay base em ms
 * @returns {Promise}
 */
const retryWithExponentialBackoff = async (fn, maxRetries = 3, baseDelay = 1000) => {
  let lastError;
  
  for (let attempt = 0; attempt <= maxRetries; attempt++) {
    try {
      return await fn();
    } catch (error) {
      lastError = error;
      
      if (attempt < maxRetries) {
        const delay = baseDelay * Math.pow(2, attempt);
        console.log(`‚ö†Ô∏è  Tentativa ${attempt + 1} falhou. Retry em ${delay}ms...`);
        await new Promise(resolve => setTimeout(resolve, delay));
      }
    }
  }
  
  throw lastError;
};

module.exports = {
  initializeVertexAI,
  transcribeAudio,
  analyzeEmotionAndNuance,
  crossReferenceOutputs,
  retryWithExponentialBackoff,
  detectAudioEncoding
};

